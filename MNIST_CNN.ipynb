{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data with a convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset was downloaded from kaggle.com in .csv format.\n",
    "\n",
    "train.csv: contains 42000 train example. Each row in the csv file contains the digit label in the first column, and 784 pixels values of the image in the following columns.\n",
    "\n",
    "Before building a CNN to classify the MNIST data, I tried using both a random forest and a fully-connected neural networks. The accuracies were 96.4% for the random forest, and 96.6% for the neural network. \n",
    "\n",
    "However, neither the random forest not the neural network take advantages of the image-specific properties of our data, i.e., low-level edges and shapes in the pictures. Thus, a CNN should be advantegous over both previous methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and convert to numpy arrays 'labels' and 'train':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"~/Python/Kaggle Digits/train.csv\")\n",
    "labels = dataset['label'].values\n",
    "train = dataset.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (42000, 784)\n",
      "Labels: (42000,)\n"
     ]
    }
   ],
   "source": [
    "print 'Train set: ' + str(train.shape)\n",
    "print 'Labels: ' + str(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Data to zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalizeData(X): \n",
    "    #input: X: 2D-numpy array with shape (#datapoints, #features)\n",
    "    #output: normalized X\n",
    "    return (X -128.0)/255.0\n",
    "X = normalizeData(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a neural network, we need to reformat our labels (0,1,2,3, ...9) into one-hot encoded labels, i.e.:\n",
    "label 1 is represented as [1.0, 0.0, 0.0 ...], \n",
    "label 2 is represented as [0.0, 1.0, 0.0 ...], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before reformating: (42000,)\n",
      "Labels after reformating:  (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "def reformatLabels(labels):\n",
    "    #input: 1D numpy array labels with shape (#samples)\n",
    "    #output: 2D numpy array of encoded labels with shape (#samples, #labels)\n",
    "    num_labels = len(np.unique(labels))\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "print 'Labels before reformating: ' + str(labels.shape)\n",
    "labels = reformatLabels(labels)\n",
    "print 'Labels after reformating:  ' + str(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the train data X is in the shape of (#samples, #pixels). However, for a CNN with Tensorflow, we  need the input data to be in the shape of (#samples, imagesize, imagesize, #channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before reformating: (42000, 784)\n",
      "Data after reformating:  (42000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def reformatData(X, num_channels = 1):\n",
    "    #input: X: 2D-numpy array with shape (#datapoints, #features)\n",
    "    #input: num_channels: integer, number of RGB channels.\n",
    "    image_size  = int(math.sqrt(X.shape[1]))\n",
    "    X = X.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    #Note: since Tensorflow cannot deal with numpy float 64 array, we convert it to float32\n",
    "    return X\n",
    "print 'Data before reformating: ' + str(X.shape)\n",
    "X = reformatData(X)\n",
    "print 'Data after reformating:  ' + str(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that reformating to images went well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11cec8790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNYQT3+FYQ/x/gty7tmsRLeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11caabdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X[10, :,:,0], cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into random test and train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37800, 28, 28, 1)\n",
      "(37800, 10)\n",
      "(4200, 28, 28, 1)\n",
      "(4200, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=0)\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables for CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size  = 28     #image size of MNIST data: 28x28 pixels\n",
    "num_labels = 10      #number of classes in the data set\n",
    "num_channels = 1     #number of RGB channels\n",
    "batch_size = 128     #batch used in each training pass\n",
    "learningrate = 0.001 #learning rate \n",
    "alpha =  0.002       #regularization strength\n",
    "train_epochs = 100   #for how many epochs we want to train the network\n",
    "totalsteps = int((train_epochs * X_train.shape[0])/batch_size) #calculate #train passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #function for initializtion of weights and biases\n",
    "    def weight_variable(shape):\n",
    "        return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1))\n",
    "    def bias_variable(shape):\n",
    "        return tf.Variable(tf.constant(0.1, shape = shape))\n",
    "\n",
    "    #maxpool defintion    \n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # define Input data dimensions\n",
    "    tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_X_test = tf.constant(X_test)\n",
    "    \n",
    "    #network definition\n",
    "    #layer 1\n",
    "    #3x3 convolution filter, 16 feature maps\n",
    "    patch_size1 = 3\n",
    "    depth1 = 16\n",
    "    layer1_weights = weight_variable([patch_size1, patch_size1, num_channels, depth1])\n",
    "    layer1_biases = bias_variable([depth1])\n",
    "    \n",
    "    #layer 2\n",
    "    #3x3 convolution filter, 16 feature maps \n",
    "    patch_size2 = 3\n",
    "    depth2 = 16\n",
    "    layer2_weights = weight_variable([patch_size2, patch_size2, depth1, depth2])\n",
    "    layer2_biases =  bias_variable([depth2])\n",
    "    \n",
    "    #layer 3\n",
    "    #5x5 convolution filter, 32 feature maps\n",
    "    patch_size3 = 5\n",
    "    depth3 = 32    \n",
    "    layer3_weights = weight_variable([patch_size3, patch_size3, depth2, depth3])\n",
    "    layer3_biases = bias_variable([depth3])\n",
    "\n",
    "    #layer 4\n",
    "    #fully connected layer 1\n",
    "    num_hidden = 1024\n",
    "    layer4_weights = weight_variable([image_size//4 * image_size//4  * depth3, num_hidden])\n",
    "    layer4_biases =  bias_variable([num_hidden])\n",
    "    \n",
    "    #layer 5\n",
    "    #fully connected layer\n",
    "    num_hidden2 = 1024\n",
    "    layer4a_weights = weight_variable([num_hidden, num_hidden2])\n",
    "    layer4a_biases =  bias_variable([num_hidden2])\n",
    "\n",
    "    #output layer\n",
    "    layer5_weights = weight_variable([num_hidden2, num_labels])\n",
    "    layer5_biases = bias_variable([num_labels])\n",
    "    \n",
    "    #make a list of the weights\n",
    "    weights = [layer1_weights,  layer2_weights, layer3_weights,  layer4_weights, layer4a_weights, layer5_weights ]\n",
    "  \n",
    "\n",
    "  # Model definition (= forward propagation)\n",
    "    def model(data, dropout = False):\n",
    "        if dropout == True:\n",
    "            keepprob = 0.5\n",
    "        else:\n",
    "            keepprob = 1.0\n",
    "            \n",
    "        #layer C1\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') \n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "\n",
    "        #layer C2\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        hidden = max_pool_2x2(hidden)\n",
    "\n",
    "        #layer C3\n",
    "        conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "        hidden = max_pool_2x2(hidden)\n",
    "        \n",
    "        #reshape for FC layer\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        #layer F4\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "        hidden = tf.nn.dropout(hidden, keepprob)\n",
    "\n",
    "        #layer F4a\n",
    "        hidden = tf.nn.relu(tf.matmul(hidden, layer4a_weights) + layer4a_biases)\n",
    "        hidden = tf.nn.dropout(hidden, keepprob)\n",
    "\n",
    "        #output layer\n",
    "        return tf.matmul(hidden, layer5_weights) + layer5_biases  \n",
    "  \n",
    "    # Define Training computation.\n",
    "    # note: dropout = True\n",
    "    logits = model(tf_X_train, dropout = True)\n",
    "    \n",
    "    # loss function for regularization\n",
    "    l2 = tf.nn.l2_loss(weights[0])  #initialize\n",
    "    for i in range(1, len(weights)):\n",
    "        l2 += tf.nn.l2_loss(weights[i])\n",
    "\n",
    "    #loss function of forward pass\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_y_train)) + alpha*l2\n",
    "       \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False) # count the number of steps taken\n",
    "    #learning_rate = tf.train.exponential_decay(learningrate, global_step, 3000, 0.9, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #\n",
    "    #note: for Adam, the learning rate is decreasing with time. Thus, we don't explicitly need to set up\n",
    "    #learning rate decay\n",
    "    optimizer = tf.train.AdamOptimizer(learningrate).minimize(loss, global_step=global_step)  \n",
    " \n",
    " \n",
    "    #Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(model(tf_X_train))\n",
    "    test_prediction = tf.nn.softmax(model(tf_X_test))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to define a function that yields the input/indices for each batch of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yield_next_batch_generator(sizeX, batch_size):\n",
    "    #Generator function which returns indices of samples (batches) used for each training pass\n",
    "    \n",
    "    currentindex = 0\n",
    "    indeces = np.arange(0,sizeX) \n",
    "    np.random.shuffle(indeces)\n",
    "    while True:  \n",
    "        start = currentindex\n",
    "        currentindex += batch_size\n",
    "        # when all trainig data have been already used, it is shuffled and currentindex is reset\n",
    "        if currentindex > sizeX:\n",
    "            np.random.shuffle(indeces)\n",
    "            start = 0\n",
    "            currentindex = batch_size\n",
    "        end = currentindex\n",
    "        yield indeces[start:end] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define an metric for the classifier. For this problem, we can use classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Training for a total number of steps of: 29531\n",
      "Minibatch loss at step 0: 36.703938\n",
      "Minibatch accuracy: 10.9%\n",
      "Test accuracy: 9.1%\n",
      " \n",
      "Minibatch loss at step 500: 4.313422\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 1000: 1.218288\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 1500: 0.641536\n",
      "Minibatch accuracy: 95.3%\n",
      "Minibatch loss at step 2000: 0.293112\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 2500: 0.234806\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 0.211172\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 3500: 0.227437\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 4000: 0.202367\n",
      "Minibatch accuracy: 97.7%\n",
      "Test accuracy: 98.3%\n",
      " \n",
      "Minibatch loss at step 4500: 0.240967\n",
      "Minibatch accuracy: 96.9%\n",
      "Minibatch loss at step 5000: 0.257879\n",
      "Minibatch accuracy: 96.9%\n",
      "Minibatch loss at step 5500: 0.155636\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 6000: 0.142717\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 98.7%\n",
      " \n",
      "Minibatch loss at step 6500: 0.156220\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 7000: 0.188557\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 7500: 0.235635\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 8000: 0.170113\n",
      "Minibatch accuracy: 99.2%\n",
      "Test accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 8500: 0.180467\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 9000: 0.197771\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 9500: 0.204936\n",
      "Minibatch accuracy: 96.9%\n",
      "Minibatch loss at step 10000: 0.142768\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 98.7%\n",
      " \n",
      "Minibatch loss at step 10500: 0.154500\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 11000: 0.216959\n",
      "Minibatch accuracy: 97.7%\n",
      "Minibatch loss at step 11500: 0.163693\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 12000: 0.147951\n",
      "Minibatch accuracy: 99.2%\n",
      "Test accuracy: 99.0%\n",
      " \n",
      "Minibatch loss at step 12500: 0.188249\n",
      "Minibatch accuracy: 96.1%\n",
      "Minibatch loss at step 13000: 0.143937\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 13500: 0.152300\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 14000: 0.144904\n",
      "Minibatch accuracy: 98.4%\n",
      "Test accuracy: 99.0%\n",
      " \n",
      "Minibatch loss at step 14500: 0.127514\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 15000: 0.152708\n",
      "Minibatch accuracy: 97.7%\n",
      "Minibatch loss at step 15500: 0.127909\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.179220\n",
      "Minibatch accuracy: 99.2%\n",
      "Test accuracy: 99.1%\n",
      " \n",
      "Minibatch loss at step 16500: 0.132061\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 17000: 0.206680\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 17500: 0.124833\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 18000: 0.137685\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 98.9%\n",
      " \n",
      "Minibatch loss at step 18500: 0.128579\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 19000: 0.123438\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 19500: 0.166544\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 20000: 0.170010\n",
      "Minibatch accuracy: 98.4%\n",
      "Test accuracy: 98.8%\n",
      " \n",
      "Minibatch loss at step 20500: 0.125749\n",
      "Minibatch accuracy: 97.7%\n",
      "Minibatch loss at step 21000: 0.114272\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 21500: 0.127631\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 22000: 0.115302\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 98.7%\n",
      " \n",
      "Minibatch loss at step 22500: 0.170601\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 23000: 0.121134\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 23500: 0.139250\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 24000: 0.120602\n",
      "Minibatch accuracy: 99.2%\n",
      "Test accuracy: 98.8%\n",
      " \n",
      "Minibatch loss at step 24500: 0.155165\n",
      "Minibatch accuracy: 97.7%\n",
      "Minibatch loss at step 25000: 0.206117\n",
      "Minibatch accuracy: 99.2%\n",
      "Minibatch loss at step 25500: 0.163347\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 26000: 0.118445\n",
      "Minibatch accuracy: 99.2%\n",
      "Test accuracy: 98.9%\n",
      " \n",
      "Minibatch loss at step 26500: 0.155631\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 27000: 0.121861\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 27500: 0.165372\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 28000: 0.137189\n",
      "Minibatch accuracy: 99.2%\n",
      "Test accuracy: 98.8%\n",
      " \n",
      "Minibatch loss at step 28500: 0.127154\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 29000: 0.175334\n",
      "Minibatch accuracy: 98.4%\n",
      "Minibatch loss at step 29500: 0.228909\n",
      "Minibatch accuracy: 96.9%\n",
      "Test accuracy: 98.6%\n",
      "Done training!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    print 'Training for a total number of steps of: ' + str(totalsteps) \n",
    "    \n",
    "    #Initialize generator:\n",
    "    g = yield_next_batch_generator(X_train.shape[0], batch_size)\n",
    "    #start training loop\n",
    "    for step in xrange(totalsteps):\n",
    "        indeces = g.next() #indeces from generator\n",
    "        feed_dict = {tf_X_train : X_train[indeces, :], tf_y_train : y_train[indeces, :]}\n",
    "\n",
    "        # Run forward and backward pass and update weights\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        #print global_step\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, y_train[indeces, :]))\n",
    "        if (step % 2000 == 0):\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), y_test))\n",
    "            print ' ' \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), y_test))\n",
    "    print 'Done training!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the CNN, we get an accuracy of 98.6% - 99.0% on the testset. That is definitely an improvement over the random forest and fully connected neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
