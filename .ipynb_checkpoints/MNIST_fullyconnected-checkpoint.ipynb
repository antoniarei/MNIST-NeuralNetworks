{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data with a fully connected neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset was downloaded from kaggle.com in .csv format.\n",
    "\n",
    "train.csv: contains 42000 train example. Each row in the csv file contains the digit label in the first column, and 784 pixels values of the image in the following columns.\n",
    "\n",
    "When classifying this data with a random forest classifier, we achieved a accuract of ca 96.4% on the test set. Let's see if one can do better with a fully connected neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and convert to numpy arrays 'labels' and 'train':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"~/Python/Kaggle Digits/train.csv\")\n",
    "labels = dataset['label'].values\n",
    "train = dataset.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (42000, 784)\n",
      "Labels: (42000,)\n"
     ]
    }
   ],
   "source": [
    "print 'Train set: ' + str(train.shape)\n",
    "print 'Labels: ' + str(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Data to zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalizeData(X): \n",
    "    #input: X: 2D-numpy array with shape (#datapoints, #features)\n",
    "    #output: normalized X\n",
    "    return (X -128.0)/255.0\n",
    "X = normalizeData(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a neural network, we need to reformat our labels (0,1,2,3, ...9) into one-hot encoded labels, i.e.:\n",
    "label 1 is represented as [1.0, 0.0, 0.0 ...], \n",
    "label 2 is represented as [0.0, 1.0, 0.0 ...], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "def reformatLabels(labels):\n",
    "    #input: 1D numpy array labels with shape (#samples)\n",
    "    #output: 2D numpy array of encoded labels with shape (#samples, #labels)\n",
    "    num_labels = len(np.unique(labels))\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "labels = reformatLabels(labels)\n",
    "print 'Labels: ' + str(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, Tensorflow cannot deal with numpy float 64 array, thus we need to convert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into random test and train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can build the network architecture in the tensorflow graph, we need to define some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size  = 28     #image size of MNIST data: 28x28 pixels\n",
    "num_labels = 10      #number of classes in the data set\n",
    "batch_size = 128     #batch used in each training pass\n",
    "learningrate = 0.02  #learning rate \n",
    "alpha =  0.0002      #regularization strength\n",
    "train_epochs = 200    #for how many epochs we want to train the network\n",
    "totalsteps = int((train_epochs * X_train.shape[0])/batch_size) #calculate #train passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    def weight_variable(shape):\n",
    "        #returns weight variables of shape shape, normally distributed\n",
    "        return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1))\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        #returns bias variables of shape shape, of constant value 0.1\n",
    "        return tf.Variable(tf.constant(0.1, shape = shape))\n",
    "\n",
    "    # Create placeholders for batches of input data\n",
    "    tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    # Create constant for test data\n",
    "    tf_X_test = tf.constant(X_test)\n",
    "    \n",
    "    # define neural network: input layer of image_size * image_size, then\n",
    "    # two layers of 1024 nodes, and then the output layer of size num_labels\n",
    "    layersetup = [image_size * image_size, 1024, 1024, num_labels]\n",
    " \n",
    "    # initialize weights and biases\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(layersetup)-1):\n",
    "        weights.append(weight_variable([layersetup[i], layersetup[i+1]]))\n",
    "        biases.append(bias_variable([layersetup[i+1]]))\n",
    "   \n",
    "    #Forward propagation through network: \n",
    "    def forwardProp(X, dropout = False):\n",
    "        inputlayer = X\n",
    "        for i in range(len(weights)-1):\n",
    "            h0 = tf.matmul(inputlayer, weights[i]) + biases[i]  #multiply\n",
    "            h1 = tf.nn.relu(h0)  #RELU nonlinearity\n",
    "            if dropout == True:\n",
    "                h1 = tf.nn.dropout(h1, 0.7)\n",
    "            inputlayer = h1\n",
    "        outputlayer= tf.matmul(inputlayer, weights[-1]) + biases[-1]\n",
    "        if dropout == True:\n",
    "            outputlayer = tf.nn.dropout(outputlayer, 0.7)\n",
    "        return outputlayer\n",
    "    \n",
    "    # Calculate forward propagation for training: \n",
    "    # (dropout usually only needed for large networks, so it probably won't improve the performance in our case)  \n",
    "    logits = forwardProp(tf_X_train, dropout = True) \n",
    "    \n",
    "    # calculate regularization loss (weight loss):\n",
    "    l2 = tf.nn.l2_loss(weights[0])\n",
    "    for i in range(1, len(weights)):\n",
    "        l2 += tf.nn.l2_loss(weights[i])\n",
    "    # total loss functions: data loss + weight loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_y_train) + alpha*l2)\n",
    "    \n",
    "    # Set up learning rate decay\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken\n",
    "    learning_rate = tf.train.exponential_decay(learningrate, global_step, 1000, 0.9)\n",
    "\n",
    "    # Optimizer to minimize the loss function which we defined above\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training and test data.\n",
    "    # Note: dropout only used for training, not for prediction!\n",
    "    train_prediction = tf.nn.softmax(forwardProp(tf_X_train, dropout = False))\n",
    "    test_prediction = tf.nn.softmax(forwardProp(tf_X_test, dropout = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to define a function that yields the input/indices for each batch of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yield_next_batch_generator(sizeX, batch_size):\n",
    "    #Generator function which returns indices of samples (batches) used for each training pass\n",
    "    \n",
    "    currentindex = 0\n",
    "    indeces = np.arange(0,sizeX) \n",
    "    np.random.shuffle(indeces)\n",
    "    while True:  \n",
    "        start = currentindex\n",
    "        currentindex += batch_size\n",
    "        # when all trainig data have been already used, it is shuffled and currentindex is reset\n",
    "        if currentindex > sizeX:\n",
    "            np.random.shuffle(indeces)\n",
    "            start = 0\n",
    "            currentindex = batch_size\n",
    "        end = currentindex\n",
    "        yield indeces[start:end] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define an metric for the classifier. For this problem, we can use classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Training for a total number of steps of: 59062\n",
      "Minibatch loss at step 0: 18.380714\n",
      "Minibatch accuracy: 4.7%\n",
      " \n",
      "Test accuracy: 10.0%\n",
      " \n",
      "Minibatch loss at step 500: 2.247907\n",
      "Minibatch accuracy: 93.8%\n",
      " \n",
      "Minibatch loss at step 1000: 2.114293\n",
      "Minibatch accuracy: 94.5%\n",
      " \n",
      "Minibatch loss at step 1500: 2.149573\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 2000: 2.263469\n",
      "Minibatch accuracy: 93.8%\n",
      " \n",
      "Test accuracy: 94.2%\n",
      " \n",
      "Minibatch loss at step 2500: 2.040787\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 3000: 2.015380\n",
      "Minibatch accuracy: 95.3%\n",
      " \n",
      "Minibatch loss at step 3500: 2.123134\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 4000: 1.974600\n",
      "Minibatch accuracy: 95.3%\n",
      " \n",
      "Test accuracy: 95.3%\n",
      " \n",
      "Minibatch loss at step 4500: 2.001168\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 5000: 1.856049\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 5500: 1.983018\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 6000: 1.896112\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 95.6%\n",
      " \n",
      "Minibatch loss at step 6500: 1.927930\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 7000: 1.823867\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 7500: 1.835600\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 8000: 2.071553\n",
      "Minibatch accuracy: 96.9%\n",
      " \n",
      "Test accuracy: 95.9%\n",
      " \n",
      "Minibatch loss at step 8500: 1.876541\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 9000: 1.911003\n",
      "Minibatch accuracy: 96.9%\n",
      " \n",
      "Minibatch loss at step 9500: 1.865686\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 10000: 1.965109\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 10500: 1.761243\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Minibatch loss at step 11000: 1.941994\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 11500: 1.777337\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 12000: 2.015696\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Test accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 12500: 1.921432\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 13000: 1.852775\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 13500: 1.808482\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 14000: 1.893125\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.3%\n",
      " \n",
      "Minibatch loss at step 14500: 1.895380\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 15000: 1.819496\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 15500: 1.823459\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 16000: 1.851381\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Test accuracy: 96.3%\n",
      " \n",
      "Minibatch loss at step 16500: 2.025459\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 17000: 1.759429\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 17500: 1.830520\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 18000: 1.825135\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Test accuracy: 96.4%\n",
      " \n",
      "Minibatch loss at step 18500: 2.002962\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 19000: 1.840720\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Minibatch loss at step 19500: 1.809545\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 20000: 1.878226\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Test accuracy: 96.5%\n",
      " \n",
      "Minibatch loss at step 20500: 1.836979\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 21000: 1.882454\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 21500: 1.712631\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Minibatch loss at step 22000: 1.866929\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Test accuracy: 96.5%\n",
      " \n",
      "Minibatch loss at step 22500: 1.938288\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 23000: 1.804932\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 23500: 1.790643\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 24000: 1.993698\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Test accuracy: 96.5%\n",
      " \n",
      "Minibatch loss at step 24500: 1.916485\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 25000: 1.922878\n",
      "Minibatch accuracy: 96.9%\n",
      " \n",
      "Minibatch loss at step 25500: 1.740903\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 26000: 1.912985\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Test accuracy: 96.5%\n",
      " \n",
      "Minibatch loss at step 26500: 1.827384\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 27000: 1.861956\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 27500: 1.906510\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 28000: 1.806391\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 28500: 1.982084\n",
      "Minibatch accuracy: 95.3%\n",
      " \n",
      "Minibatch loss at step 29000: 1.889682\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 29500: 1.907966\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 30000: 1.888583\n",
      "Minibatch accuracy: 94.5%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 30500: 1.810014\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 31000: 1.723566\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 31500: 1.786711\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 32000: 1.902088\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 32500: 1.880872\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 33000: 1.846478\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Minibatch loss at step 33500: 1.746675\n",
      "Minibatch accuracy: 96.9%\n",
      " \n",
      "Minibatch loss at step 34000: 1.853949\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 34500: 1.876742\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 35000: 1.873341\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 35500: 1.730285\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 36000: 1.934837\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 36500: 1.756635\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 37000: 1.896795\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 37500: 1.818814\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 38000: 2.030911\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 38500: 1.823735\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 39000: 1.788087\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 39500: 1.756091\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 40000: 1.867946\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 40500: 1.938740\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 41000: 1.869404\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 41500: 1.894502\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 42000: 1.826593\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 42500: 1.753692\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Minibatch loss at step 43000: 1.829608\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 43500: 1.919952\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Minibatch loss at step 44000: 1.960827\n",
      "Minibatch accuracy: 96.9%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 44500: 1.837021\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 45000: 1.927597\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 45500: 1.794456\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 46000: 1.744465\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 46500: 1.759350\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Minibatch loss at step 47000: 1.976581\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 47500: 1.903409\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 48000: 1.890125\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 48500: 1.817751\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 49000: 1.850787\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 49500: 1.812819\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 50000: 1.903091\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 50500: 1.830434\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Minibatch loss at step 51000: 1.817814\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 51500: 1.880824\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 52000: 1.863968\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 52500: 1.915798\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 53000: 1.892609\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Minibatch loss at step 53500: 1.670850\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 54000: 1.786470\n",
      "Minibatch accuracy: 97.7%\n",
      " \n",
      "Test accuracy: 96.7%\n",
      " \n",
      "Minibatch loss at step 54500: 1.801303\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 55000: 1.782575\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 55500: 1.808394\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 56000: 1.880101\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Test accuracy: 96.7%\n",
      " \n",
      "Minibatch loss at step 56500: 1.982252\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 57000: 1.812383\n",
      "Minibatch accuracy: 96.1%\n",
      " \n",
      "Minibatch loss at step 57500: 1.872516\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 58000: 1.796641\n",
      "Minibatch accuracy: 100.0%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      " \n",
      "Minibatch loss at step 58500: 1.806609\n",
      "Minibatch accuracy: 99.2%\n",
      " \n",
      "Minibatch loss at step 59000: 1.734342\n",
      "Minibatch accuracy: 98.4%\n",
      " \n",
      "Test accuracy: 96.6%\n",
      "Done training!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print 'Initialized'\n",
    "    print 'Training for a total number of steps of: ' + str(totalsteps) \n",
    "    #Initialize generator:\n",
    "    g = yield_next_batch_generator(X_train.shape[0], batch_size)\n",
    "    for step in xrange(totalsteps):\n",
    "        indeces = g.next() #indeces from generator\n",
    "        feed_dict = {tf_X_train : X_train[indeces, :], tf_y_train : y_train[indeces, :]}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, y_train[indeces, :]))\n",
    "            print ' '\n",
    "        if (step % 2000 == 0):\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), y_test))\n",
    "            print ' ' \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), y_test))\n",
    "    print 'Done training!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the fully-connected neural network, we get an accuracy of ca 96.6% on the test set. This is similar to the performance of the random forest classifier (ca. 96.4%), and given that the neural network takes a significantly longer time to train, it does not seem to advantageous to the random forest. \n",
    "\n",
    "I also tried to tune the network parameters, e.g., learning rate, learning rate decay rate, #nodes, #layers, batchsize, but I did not get a better performance than ca 96.6%. However, I did not spend much time on it, so one could possibly obtain a better performance than, e.g., by trying to use a different optimizer, train for a longer time, or find a different learning rate decay schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
